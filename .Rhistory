tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
# creating vocabulary and document-term matrix
vocab <- create_vocabulary(it_tweets)
vectorizer <- vocab_vectorizer(vocab)
# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)
# define tf-idf model
tfidf <- TfIdf$new()
# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
# loading classification model
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
# adding rates to initial dataset
df_tweets$sentiment <- preds_tweets
}
getSentimentAnalysisDF <- function(df_tweets, glmnet_classifier) {
df_tweets %>%
# converting some symbols
dmap_at('text', conv_fun)
##### doc2vec #####
# define preprocessing function and tokenization function
prep_fun <- tolower
tok_fun <- word_tokenizer
# preprocessing and tokenization
it_tweets <- itoken(df_tweets$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
# creating vocabulary and document-term matrix
vocab <- create_vocabulary(it_tweets)
vectorizer <- vocab_vectorizer(vocab)
# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)
# define tf-idf model
tfidf <- TfIdf$new()
# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
# loading classification model
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
# adding rates to initial dataset
df_tweets$sentiment <- preds_tweets
}
getSentimentAnalysisDF <- function(df_tweets, glmnet_classifier=NULL) {
df_tweets %>%
# converting some symbols
dmap_at('text', conv_fun)
##### doc2vec #####
# define preprocessing function and tokenization function
prep_fun <- tolower
tok_fun <- word_tokenizer
# preprocessing and tokenization
it_tweets <- itoken(df_tweets$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
# creating vocabulary and document-term matrix
vocab <- create_vocabulary(it_tweets)
vectorizer <- vocab_vectorizer(vocab)
# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)
# define tf-idf model
tfidf <- TfIdf$new()
# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
# loading classification model
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
# adding rates to initial dataset
df_tweets$sentiment <- preds_tweets
return(df_tweets)
}
df_tweets <- getSentimentAnalysisDF(df_tweets)
getSentimentAnalysisDF <- function(df_tweets, glmnet_classifier=NULL) {
# function for converting some symbols
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")
df_tweets %>%
# converting some symbols
dmap_at('text', conv_fun)
##### doc2vec #####
# define preprocessing function and tokenization function
prep_fun <- tolower
tok_fun <- word_tokenizer
# preprocessing and tokenization
it_tweets <- itoken(df_tweets$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
# creating vocabulary and document-term matrix
vocab <- create_vocabulary(it_tweets)
vectorizer <- vocab_vectorizer(vocab)
# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)
# define tf-idf model
tfidf <- TfIdf$new()
# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
# loading classification model
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
# adding rates to initial dataset
df_tweets$sentiment <- preds_tweets
return(df_tweets)
}
df_tweets <- getSentimentAnalysisDF(df_tweets)
getSentimentAnalysisDF <- function(df_tweets, glmnet_classifier=NULL) {
# function for converting some symbols
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")
df_tweets %>%
# converting some symbols
dmap_at('text', conv_fun)
##### doc2vec #####
# define preprocessing function and tokenization function
prep_fun <- tolower
tok_fun <- word_tokenizer
# preprocessing and tokenization
it_tweets <- itoken(df_tweets$tweet,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
# creating vocabulary and document-term matrix
vocab <- create_vocabulary(it_tweets)
vectorizer <- vocab_vectorizer(vocab)
# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)
# define tf-idf model
tfidf <- TfIdf$new()
# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
# loading classification model
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
# adding rates to initial dataset
df_tweets$sentiment <- preds_tweets
return(df_tweets)
}
df_tweets <- getSentimentAnalysisDF(df_tweets)
df_tweets$text <- df_tweets$tweet
df_tweets$tweet <- NULL
View(df_tweets)
getSentimentAnalysisDF <- function(df_tweets, glmnet_classifier=NULL) {
# function for converting some symbols
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")
df_tweets %>%
# converting some symbols
dmap_at('text', conv_fun)
##### doc2vec #####
# define preprocessing function and tokenization function
prep_fun <- tolower
tok_fun <- word_tokenizer
# preprocessing and tokenization
it_tweets <- itoken(df_tweets$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
# creating vocabulary and document-term matrix
vocab <- create_vocabulary(it_tweets)
vectorizer <- vocab_vectorizer(vocab)
# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)
# define tf-idf model
tfidf <- TfIdf$new()
# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
# loading classification model
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
# adding rates to initial dataset
df_tweets$sentiment <- preds_tweets
return(df_tweets)
}
df_tweets <- getSentimentAnalysisDF(df_tweets)
xx <- read.csv("testdata.manual.2009.06.14.csv")
xx <- read.csv("data/testdata.manual.2009.06.14.csv")
View(xx)
xx <- read.csv("data/training.1600000.processed.noemoticon.csv")
setup_twitter_oauth(consumer_key='1b8sIKPv3yYjXteh8UW4BCYaZ',
consumer_secret='dUTMMhAoMDAsjABDq0NoUQasFeAkFZ1Q7RPJqfzligZw8bMrIF',
access_token='3091224267-svuyKLlhEFuwE8eOTkcpZqyKUuFz24aayr6wIa0',
access_secret='YvHamKBCkkN5FAPiNkFkQNe04jkYF0q60tR66rHmhafXj')
library(twitteR)
setup_twitter_oauth(consumer_key='1b8sIKPv3yYjXteh8UW4BCYaZ',
consumer_secret='dUTMMhAoMDAsjABDq0NoUQasFeAkFZ1Q7RPJqfzligZw8bMrIF',
access_token='3091224267-svuyKLlhEFuwE8eOTkcpZqyKUuFz24aayr6wIa0',
access_secret='YvHamKBCkkN5FAPiNkFkQNe04jkYF0q60tR66rHmhafXj')
df_tweets <- twListToDF(searchTwitter('setapp OR #setapp', n = 1000, lang = 'en')) %>%
# converting some symbols
dmap_at('text', conv_fun)
View(df_tweets)
df_tweets <- twListToDF(searchTwitter('setapp OR #setapp', n = 1000, lang = 'en')) %>%
# converting some symbols
dmap_at('text', conv_fun)
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")
df_tweets <- twListToDF(searchTwitter('setapp OR #setapp', n = 1000, lang = 'en')) %>%
# converting some symbols
dmap_at('text', conv_fun)
View(df_tweets)
prep_fun <- tolower
tok_fun <- word_tokenizer
it_tweets <- itoken(df_tweets$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
vocab <- create_vocabulary(it_tweets)
vectorizer <- vocab_vectorizer(vocab)
dtm_tweets <- create_dtm(it_tweets, vectorizer)
tfidf <- TfIdf$new()
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
glmnet_classifier <- readRDS('glmnet_classifier.RDS')
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
tweets_classified <- read_csv('data/training.1600000.processed.noemoticon.csv',
col_names = c('sentiment', 'id', 'date', 'query', 'user', 'text')) %>%
# converting some symbols
dmap_at('text', conv_fun) %>%
# replacing class values
mutate(sentiment = ifelse(sentiment == 0, 0, 1))
tweets_classified <- read_csv('data/training.1600000.processed.noemoticon.csv',
col_names = c('sentiment', 'id', 'date', 'query', 'user', 'text')) %>%
# converting some symbols
dmap_at('text', conv_fun) %>%
# replacing class values
mutate(sentiment = ifelse(sentiment == 0, 0, 1))
tweets_classified <- read_csv('data/training.1600000.processed.noemoticon.csv',
col_names = c('sentiment', 'id', 'date', 'query', 'user', 'text')) %>%
# converting some symbols
dmap_at('text', conv_fun) %>%
# replacing class values
mutate(sentiment = ifelse(sentiment == 0, 0, 1))
set.seed(2340)
trainIndex <- createDataPartition(tweets_classified$sentiment, p = 0.8,
list = FALSE,
times = 1)
tweets_train <- tweets_classified[trainIndex, ]
tweets_test <- tweets_classified[-trainIndex, ]
prep_fun <- tolower
tok_fun <- word_tokenizer
it_train <- itoken(tweets_train$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = tweets_train$id,
progressbar = TRUE)
it_test <- itoken(tweets_test$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = tweets_test$id,
progressbar = TRUE)
vocab <- create_vocabulary(it_train)
vectorizer <- vocab_vectorizer(vocab)
dtm_tweets <- create_dtm(it_tweets, vectorizer)
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
saveRDS(vectorizer, 'model/vectorizer.RDS')
df_tweets <- readRDS('data/df_tweets.RDS')
dmap_at(df_tweets,'text', conv_fun)
df_tweets <- dmap_at(df_tweets,'text', conv_fun)
View(df_tweets)
it_tweets <- itoken(df_tweets$tweet,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
dtm_tweets <- create_dtm(it_tweets, vectorizer)
tfidf <- TfIdf$new()
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
df_tweets$sentiment <- preds_tweets
View(df_tweets)
getSentimentAnalysisDF <- function(df_tweets, glmnet_classifier=NULL) {
# function for converting some symbols
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")
df_tweets %>%
# converting some symbols
dmap_at('text', conv_fun)
##### doc2vec #####
# define preprocessing function and tokenization function
prep_fun <- tolower
tok_fun <- word_tokenizer
# preprocessing and tokenization
it_tweets <- itoken(df_tweets$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
# get vectorizer that was used for developing model
vectorizer <- readRDS('model/vectorizer.RDS')
# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)
# define tf-idf model
tfidf <- TfIdf$new()
# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
# loading classification model
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
# adding rates to initial dataset
df_tweets$sentiment <- preds_tweets
return(df_tweets)
}
df_tweets <- readRDS('data/df_tweets.RDS')
View(df_tweets)
getSentimentAnalysisDF <- function(df_tweets, glmnet_classifier=NULL) {
# function for converting some symbols
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")
df_tweets %>%
# converting some symbols
dmap_at('text', conv_fun)
##### doc2vec #####
# define preprocessing function and tokenization function
prep_fun <- tolower
tok_fun <- word_tokenizer
# preprocessing and tokenization
it_tweets <- itoken(df_tweets$tweet,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = df_tweets$id,
progressbar = TRUE)
# get vectorizer that was used for developing model
vectorizer <- readRDS('model/vectorizer.RDS')
# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)
# define tf-idf model
tfidf <- TfIdf$new()
# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)
# loading classification model
glmnet_classifier <- readRDS('model/glmnet_classifier.RDS')
# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]
# adding rates to initial dataset
df_tweets$sentiment <- preds_tweets
return(df_tweets)
}
df_tweets <- getSentimentAnalysisDF(df_tweets)
View(df_tweets)
getSentimentAnalysisPlot <- function(df_tweets) {
# color palette
cols <- c("#ce472e", "#f05336", "#ffd73e", "#eec73a", "#4ab04a")
set.seed(932)
samp_ind <- sample(c(1:nrow(df_tweets)), nrow(df_tweets) * 0.1) # 10% for labeling
# plotting
x <- ggplot(df_tweets, aes(x = created, y = sentiment, color = sentiment)) +
theme_minimal() +
scale_color_gradientn(colors = cols, limits = c(0, 1),
breaks = seq(0, 1, by = 1/4),
labels = c("0", round(1/4*1, 1), round(1/4*2, 1), round(1/4*3, 1), round(1/4*4, 1)),
guide = guide_colourbar(ticks = T, nbin = 50, barheight = .5, label = T, barwidth = 10)) +
geom_point(aes(color = sentiment), alpha = 0.8) +
geom_hline(yintercept = 0.65, color = "#4ab04a", size = 1.5, alpha = 0.6, linetype = "longdash") +
geom_hline(yintercept = 0.35, color = "#f05336", size = 1.5, alpha = 0.6, linetype = "longdash") +
geom_smooth(size = 1.2, alpha = 0.2) +
geom_label_repel(data = df_tweets[samp_ind, ],
aes(label = round(sentiment, 2)),
fontface = 'bold',
size = 2.5,
max.iter = 100) +
theme(legend.position = 'bottom',
legend.direction = "horizontal",
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
plot.title = element_text(size = 20, face = "bold", vjust = 2, color = 'black', lineheight = 0.8),
axis.title.x = element_text(size = 16),
axis.title.y = element_text(size = 16),
axis.text.y = element_text(size = 8, face = "bold", color = 'black'),
axis.text.x = element_text(size = 8, face = "bold", color = 'black')) +
ggtitle("Tweets Sentiment rate (probability of positiveness)")
return(x)
}
x <- getSentimentAnalysisPlot(df_tweets)
x
View(df_tweets)
cols <- c("#ce472e", "#f05336", "#ffd73e", "#eec73a", "#4ab04a")
set.seed(932)
samp_ind <- sample(c(1:nrow(df_tweets)), nrow(df_tweets) * 0.1) # 10% for labeling
ggplot(df_tweets, aes(x = created, y = sentiment, color = sentiment)) +
theme_minimal() +
scale_color_gradientn(colors = cols, limits = c(0, 1),
breaks = seq(0, 1, by = 1/4),
labels = c("0", round(1/4*1, 1), round(1/4*2, 1), round(1/4*3, 1), round(1/4*4, 1)),
guide = guide_colourbar(ticks = T, nbin = 50, barheight = .5, label = T, barwidth = 10)) +
geom_point(aes(color = sentiment), alpha = 0.8) +
geom_hline(yintercept = 0.65, color = "#4ab04a", size = 1.5, alpha = 0.6, linetype = "longdash") +
geom_hline(yintercept = 0.35, color = "#f05336", size = 1.5, alpha = 0.6, linetype = "longdash") +
geom_smooth(size = 1.2, alpha = 0.2) +
geom_label_repel(data = df_tweets[samp_ind, ],
aes(label = round(sentiment, 2)),
fontface = 'bold',
size = 2.5,
max.iter = 100) +
theme(legend.position = 'bottom',
legend.direction = "horizontal",
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
plot.title = element_text(size = 20, face = "bold", vjust = 2, color = 'black', lineheight = 0.8),
axis.title.x = element_text(size = 16),
axis.title.y = element_text(size = 16),
axis.text.y = element_text(size = 8, face = "bold", color = 'black'),
axis.text.x = element_text(size = 8, face = "bold", color = 'black')) +
ggtitle("Tweets Sentiment rate (probability of positiveness)")
ggplot(df_tweets, aes(x = created, y = sentiment, color = sentiment))
View(df_tweets)
ggplot(df_tweets, aes(x = tweetcreated, y = sentiment, color = sentiment))
ggplot(df_tweets, aes(x = tweetcreated, y = sentiment, color = sentiment)) +
theme_minimal() +
scale_color_gradientn(colors = cols, limits = c(0, 1),
breaks = seq(0, 1, by = 1/4),
labels = c("0", round(1/4*1, 1), round(1/4*2, 1), round(1/4*3, 1), round(1/4*4, 1)),
guide = guide_colourbar(ticks = T, nbin = 50, barheight = .5, label = T, barwidth = 10)) +
geom_point(aes(color = sentiment), alpha = 0.8)
getSentimentAnalysisPlot <- function(df_tweets) {
# color palette
cols <- c("#ce472e", "#f05336", "#ffd73e", "#eec73a", "#4ab04a")
set.seed(932)
samp_ind <- sample(c(1:nrow(df_tweets)), nrow(df_tweets) * 0.1) # 10% for labeling
# plotting
x <- ggplot(df_tweets, aes(x = tweetcreated, y = sentiment, color = sentiment)) +
theme_minimal() +
scale_color_gradientn(colors = cols, limits = c(0, 1),
breaks = seq(0, 1, by = 1/4),
labels = c("0", round(1/4*1, 1), round(1/4*2, 1), round(1/4*3, 1), round(1/4*4, 1)),
guide = guide_colourbar(ticks = T, nbin = 50, barheight = .5, label = T, barwidth = 10)) +
geom_point(aes(color = sentiment), alpha = 0.8) +
geom_hline(yintercept = 0.65, color = "#4ab04a", size = 1.5, alpha = 0.6, linetype = "longdash") +
geom_hline(yintercept = 0.35, color = "#f05336", size = 1.5, alpha = 0.6, linetype = "longdash") +
geom_smooth(size = 1.2, alpha = 0.2) +
geom_label_repel(data = df_tweets[samp_ind, ],
aes(label = round(sentiment, 2)),
fontface = 'bold',
size = 2.5,
max.iter = 100) +
theme(legend.position = 'bottom',
legend.direction = "horizontal",
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
plot.title = element_text(size = 20, face = "bold", vjust = 2, color = 'black', lineheight = 0.8),
axis.title.x = element_text(size = 16),
axis.title.y = element_text(size = 16),
axis.text.y = element_text(size = 8, face = "bold", color = 'black'),
axis.text.x = element_text(size = 8, face = "bold", color = 'black')) +
ggtitle("Tweets Sentiment rate (probability of positiveness)")
return(x)
}
x <- getSentimentAnalysisPlot(df_tweets)
x
install.packages("roxygen")
install.packages("roxygen2")
library(roxygen2)
getwd()
roxygen2::roxygenize()
roxygen2::roxygenize()
df_tweets <- readRDS("df_tweets.RDS")
df_tweets <- readRDS("data/df_tweets.RDS")
View(df_tweets)
df_tweets <- getSentimentAnalysisDF(df_tweets)
df_tweets <- sentimentAnalysis::getSentimentAnalysisDF(df_tweets)
library(tidyverse)
library(text2vec)
library(caret)
library(glmnet)
library(ggrepel)
library(purrrlyr)
df_tweets <- sentimentAnalysis::getSentimentAnalysisDF(df_tweets)
View(df_tweets)
as.POSIXct(NA, format="%Y-%m-%d %H:%M:%S",tz="UTC")
library(sentimentAnalysis)
roxygen2::roxygenize()
df_tweets <- readRDS('data/df_tweets.RDS')
df_tweets <- sentimentAnalysis::getSentimentAnalysisDF(df_tweets)
library(purrrlyr)
df_tweets <- sentimentAnalysis::getSentimentAnalysisDF(df_tweets)
library(tidyverse)
library(text2vec)
library(caret)
library(glmnet)
library(ggrepel)
df_tweets <- sentimentAnalysis::getSentimentAnalysisDF(df_tweets)
roxygen2::roxygenize()
library(sentimentAnalysis)
df_tweets <- readRDS('data/df_tweets.RDS')
df_tweets <- sentimentAnalysis::getSentimentAnalysisDF(df_tweets)
library(tidyverse)
library(text2vec)
library(caret)
library(glmnet)
library(ggrepel)
library(purrrlyr)
df_tweets <- sentimentAnalysis::getSentimentAnalysisDF(df_tweets)
View(df_tweets)
library(sentimentAnalysis)
library(sentimentAnalysis)
library(sentimentAnalysis)
roxygen2::roxygenize()
library(sentimentAnalysis)
library(sentimentAnalysis)
